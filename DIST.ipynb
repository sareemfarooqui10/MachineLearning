{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yhxNjlrHWuk"
      },
      "source": [
        "#RR with 20newsgroup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaMSXR-NfkHi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def compute_odds_ratio_weights(L, classes, threshold=0.1):\n",
        "    labeled_docs, labels = zip(*L)\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_labeled = vectorizer.fit_transform(labeled_docs)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    term_weights_class = defaultdict(dict)\n",
        "    partitioned_terms = {k: {'greater': set(), 'less_equal': set()} for k in classes}\n",
        "\n",
        "    doc_freq_class = {k: Counter() for k in classes}\n",
        "    doc_freq_other = {k: Counter() for k in classes}\n",
        "\n",
        "    for doc, label in L:\n",
        "        terms = set(doc.split())\n",
        "        for term in terms:\n",
        "            for k in classes:\n",
        "                if label == k:\n",
        "                    doc_freq_class[k][term] += 1\n",
        "                else:\n",
        "                    doc_freq_other[k][term] += 1\n",
        "\n",
        "    total_docs = len(labeled_docs)\n",
        "    vocab_size = len(feature_names)\n",
        "\n",
        "    for k in classes:\n",
        "        num_class_docs = sum(1 for label in labels if label == k)\n",
        "        num_other_docs = total_docs - num_class_docs\n",
        "\n",
        "        for term in feature_names:\n",
        "            P_t_given_k = (doc_freq_class[k][term] + 1) / (num_class_docs + vocab_size)\n",
        "            P_t_given_not_k = (doc_freq_other[k][term] + 1) / (num_other_docs + vocab_size)\n",
        "\n",
        "            if abs(P_t_given_k - P_t_given_not_k) > threshold:\n",
        "                if P_t_given_k > P_t_given_not_k:\n",
        "                    partitioned_terms[k]['greater'].add(term)\n",
        "                    relative_risk = (P_t_given_k / P_t_given_not_k)\n",
        "                    term_weights_class[k][term] = relative_risk\n",
        "                else:\n",
        "                    partitioned_terms[k]['less_equal'].add(term)\n",
        "                    relative_risk = (P_t_given_not_k / P_t_given_k)\n",
        "                    term_weights_class[k][term] = relative_risk\n",
        "\n",
        "    return term_weights_class, partitioned_terms, feature_names\n",
        "\n",
        "def create_document_vectors(L, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in L:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            numerator_greater = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "            numerator_less_equal = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            X.append([doc_scores[k]['greater'], doc_scores[k]['less_equal']])\n",
        "            y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / len(L)\n",
        "    return accuracy\n",
        "\n",
        "def plot_feature_space(scores, labels, alpha_k, alpha_0, classes):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    colors = ['red', 'green', 'blue']  # Assuming 3 classes, you can adjust as necessary\n",
        "    markers = ['o', 'x', 's']  # Different markers for classes and non-classes\n",
        "\n",
        "    for idx, k in enumerate(classes):\n",
        "        class_scores = np.array([[doc_scores[k]['greater'], doc_scores[k]['less_equal']] for doc_scores in scores.values()])\n",
        "        class_labels = np.array([1 if label == k else 0 for label in labels])\n",
        "\n",
        "        plt.scatter(class_scores[class_labels == 1, 0], class_scores[class_labels == 1, 1], label=f'Class {k}', color=colors[idx], marker=markers[0])\n",
        "        plt.scatter(class_scores[class_labels == 0, 0], class_scores[class_labels == 0, 1], label=f'Not Class {k}', color=colors[idx], marker=markers[1])\n",
        "\n",
        "        # Plot the discriminating line\n",
        "        x_vals = np.linspace(min(class_scores[:, 0]), max(class_scores[:, 0]), 100)\n",
        "        y_vals = (alpha_k[k][0] * x_vals + alpha_0[k]) / alpha_k[k][1]\n",
        "        plt.plot(x_vals, y_vals, label=f'Discriminant for Class {k}', linestyle='--', color=colors[idx])\n",
        "\n",
        "    plt.xlabel('Score^C/k(x)')\n",
        "    plt.ylabel('Score^k(x)')\n",
        "    plt.legend()\n",
        "    plt.title('Feature Space with Decision Boundaries')\n",
        "    plt.show()\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "# Driver function\n",
        "\n",
        "#Train Data\n",
        "data_dir = \"20news-bydate/20news-bydate-train\"\n",
        "L, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "term_weights_class, partitioned_terms, vocab = compute_odds_ratio_weights(L, classes, threshold=0.1)\n",
        "\n",
        "significant_terms = set()\n",
        "for k in classes:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = list(significant_terms)\n",
        "\n",
        "document_vectors = create_document_vectors(L, significant_terms)\n",
        "\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms)\n",
        "\n",
        "labels = [label for _, label in L]\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, classes)\n",
        "\n",
        "accuracy = evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "plot_feature_space(scores, labels, alpha_k, alpha_0, classes)\n",
        "\n",
        " # Test Data\n",
        "data_dir = \"20news-bydate/20news-bydate-test\"\n",
        "U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "\n",
        "# # Print term weights for each class\n",
        "# for k in classes:\n",
        "#     print(f\"Class {k} term weights:\")\n",
        "#     for term, weight in term_weights_class[k].items():\n",
        "#         print(f\"  {term}: {weight}\")\n",
        "\n",
        "# # Print partitioned terms for each class\n",
        "# for k in classes:\n",
        "#     print(f\"Class {k} partitioned terms:\")\n",
        "#     print(\"  Terms with P_t_given_k > P_t_given_not_k:\")\n",
        "#     print(f\"    {partitioned_terms[k]['greater']}\")\n",
        "#     print(\"  Terms with P_t_given_k <= P_t_given_not_k:\")\n",
        "#     print(f\"    {partitioned_terms[k]['less_equal']}\")\n",
        "\n",
        "# # Print scores for each document for each class\n",
        "# print(\"Scores:\")\n",
        "# for doc_index, doc_scores in scores.items():\n",
        "#     print(f\"Document {doc_index} scores:\")\n",
        "#     for k, k_scores in doc_scores.items():\n",
        "#         print(f\"  Class {k} scores:\")\n",
        "#         print(f\"    Greater set score: {k_scores['greater']}\")\n",
        "#         print(f\"    Less_equal set score: {k_scores['less_equal']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMvPo0I1pMXn"
      },
      "source": [
        "# **START FROM HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IzJGEYijaHP"
      },
      "source": [
        "#Log of RR with conditional probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvIgKu1ZMJd8",
        "outputId": "4ef24fce-8e9c-4d36-f6cf-8bb19283df78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('term1 term2 term3 term5', 1)\n",
            "term1  in class  1  with prob of term in doc of the same class is  0.2857142857142857  and prob of term in doc which is not in k is  0.4444444444444444\n",
            "term2  in class  1  with prob of term in doc of the same class is  0.42857142857142855  and prob of term in doc which is not in k is  0.5555555555555556\n",
            "term4  in class  1  with prob of term in doc of the same class is  0.2857142857142857  and prob of term in doc which is not in k is  0.4444444444444444\n",
            "term5  in class  2  with prob of term in doc of the same class is  0.125  and prob of term in doc which is not in k is  0.375\n",
            "term2  in class  3  with prob of term in doc of the same class is  0.3333333333333333  and prob of term in doc which is not in k is  0.6\n",
            "term3  in class  3  with prob of term in doc of the same class is  0.16666666666666666  and prob of term in doc which is not in k is  0.3\n",
            "term5  in class  3  with prob of term in doc of the same class is  0.3333333333333333  and prob of term in doc which is not in k is  0.2\n",
            "Class 1 term weights:\n",
            "  term1: 0.44183275227903923\n",
            "  term2: 0.2595111954850848\n",
            "  term4: 0.44183275227903923\n",
            "Class 2 term weights:\n",
            "  term5: 1.0986122886681098\n",
            "Class 3 term weights:\n",
            "  term2: 0.5877866649021191\n",
            "  term3: 0.5877866649021191\n",
            "  term5: 0.5108256237659906\n",
            "Class 1 partitioned terms:\n",
            "  Terms with P_t_given_k > P_t_given_not_k:\n",
            "    set()\n",
            "  Terms with P_t_given_k <= P_t_given_not_k:\n",
            "    {'term4', 'term1', 'term2'}\n",
            "Class 2 partitioned terms:\n",
            "  Terms with P_t_given_k > P_t_given_not_k:\n",
            "    set()\n",
            "  Terms with P_t_given_k <= P_t_given_not_k:\n",
            "    {'term5'}\n",
            "Class 3 partitioned terms:\n",
            "  Terms with P_t_given_k > P_t_given_not_k:\n",
            "    {'term5'}\n",
            "  Terms with P_t_given_k <= P_t_given_not_k:\n",
            "    {'term2', 'term3'}\n",
            "Scores:\n",
            "Document 0 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.175335986941031\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.27465307216702745\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.12770640594149765\n",
            "    Less_equal set score: 0.29389333245105953\n",
            "Document 1 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.23378131592137466\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.0\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.3918577766014127\n",
            "Document 2 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.350671973882062\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.0\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.29389333245105953\n",
            "Document 3 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.38105890001438775\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.0\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.19592888830070634\n",
            "Document 4 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.350671973882062\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.0\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.29389333245105953\n",
            "Document 5 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.2857941750107908\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.27465307216702745\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.12770640594149765\n",
            "    Less_equal set score: 0.14694666622552977\n"
          ]
        }
      ],
      "source": [
        "#Log of RR with conditional prob, Z^k, Z^(C/k), significant terms ONLY, Score^k(x), Score^C/k(x)\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def compute_odds_ratio_weights(L, classes, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Compute the weights for each term in the vocabulary using the odds ratio method with add-one Laplace smoothing\n",
        "    and partition terms into two sets based on whether P_t_given_k > P_t_given_not_k if |P_t_given_k - P_t_given_not_k| > threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - L: List of tuples (document, label) representing the labeled documents.\n",
        "    - classes: List of possible class labels.\n",
        "    - threshold: Threshold value for |P_t_given_k - P_t_given_not_k| to include terms in the model.\n",
        "\n",
        "    Returns:\n",
        "    - term_weights_class: Dictionary where term_weights_class[k][term] gives the odds ratio of the term for class k.\n",
        "    - partitioned_terms: Dictionary where partitioned_terms[k] contains two sets: terms with P_t_given_k > P_t_given_not_k\n",
        "                         and terms with P_t_given_k <= P_t_given_not_k.\n",
        "    \"\"\"\n",
        "    # Separate documents and labels\n",
        "    labeled_docs, labels = zip(*L)\n",
        "\n",
        "    # Vectorize documents to get term frequencies\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_labeled = vectorizer.fit_transform(labeled_docs)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    term_weights_class = defaultdict(dict)\n",
        "    partitioned_terms = {k: {'greater': set(), 'less_equal': set()} for k in classes} #greater is the label for Z^k and the other is Z^(C/k)\n",
        "\n",
        "    # Count document frequencies for each term in each class\n",
        "    doc_freq_class = {k: Counter() for k in classes}\n",
        "    doc_freq_other = {k: Counter() for k in classes}\n",
        "\n",
        "    for doc, label in L:\n",
        "        terms = set(doc.split())\n",
        "        for term in terms:\n",
        "            for k in classes:\n",
        "                if label == k:\n",
        "                    doc_freq_class[k][term] += 1\n",
        "                else:\n",
        "                    doc_freq_other[k][term] += 1\n",
        "\n",
        "    total_docs = len(labeled_docs)\n",
        "    vocab_size = len(feature_names)\n",
        "\n",
        "    for k in classes:\n",
        "        num_class_docs = sum(1 for label in labels if label == k)\n",
        "        num_other_docs = total_docs - num_class_docs\n",
        "\n",
        "        for term in feature_names:\n",
        "            # Apply add-one Laplace smoothing\n",
        "            P_t_given_k = (doc_freq_class[k][term] + 1) / (num_class_docs + vocab_size)\n",
        "            P_t_given_not_k = (doc_freq_other[k][term] + 1) / (num_other_docs + vocab_size)\n",
        "\n",
        "            if abs(P_t_given_k - P_t_given_not_k) > threshold:\n",
        "                if P_t_given_k > P_t_given_not_k:\n",
        "                    print(term,\" in class \",k,\" with prob of term in doc of the same class is \",P_t_given_k,\" and prob of term in doc which is not in k is \",P_t_given_not_k)\n",
        "                    partitioned_terms[k]['greater'].add(term)\n",
        "                    relative_risk = np.log(P_t_given_k / P_t_given_not_k)\n",
        "                    term_weights_class[k][term] = relative_risk\n",
        "                else:\n",
        "                    print(term,\" in class \",k,\" with prob of term in doc of the same class is \",P_t_given_k,\" and prob of term in doc which is not in k is \",P_t_given_not_k)\n",
        "                    partitioned_terms[k]['less_equal'].add(term)\n",
        "                    relative_risk = np.log(P_t_given_not_k / P_t_given_k)\n",
        "                    term_weights_class[k][term] = relative_risk\n",
        "\n",
        "    return term_weights_class, partitioned_terms, feature_names\n",
        "\n",
        "def create_document_vectors(L, significant_terms):\n",
        "    \"\"\"\n",
        "    Create binary document vectors indicating the presence of significant terms in each document.\n",
        "\n",
        "    Parameters:\n",
        "    - L: List of tuples (document, label) representing the labeled documents.\n",
        "    - significant_terms: Set of significant terms to include in the document vectors.\n",
        "\n",
        "    Returns:\n",
        "    - document_vectors: List of binary vectors representing each document.\n",
        "    \"\"\"\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in L:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    \"\"\"\n",
        "    Calculate the score for each document for each class using the given formula separately for significant terms\n",
        "    in the 'greater' set and the 'less_equal' set of partitioned_terms.\n",
        "\n",
        "    Parameters:\n",
        "    - document_vectors: List of binary vectors representing each document.\n",
        "    - term_weights_class: Dictionary where term_weights_class[k][term] gives the odds ratio of the term for class k.\n",
        "    - significant_terms: List of significant terms to include in the document vectors.\n",
        "    - partitioned_terms: Dictionary where partitioned_terms[k] contains two sets: 'greater' and 'less_equal'.\n",
        "\n",
        "    Returns:\n",
        "    - scores: Dictionary where scores[doc_index][k] gives the score of document `doc_index` for class `k` for each set.\n",
        "    \"\"\"\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            numerator_greater = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "            numerator_less_equal = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Example usage\n",
        "L = [\n",
        "    (\"term1 term2 term3 term5\", 1),\n",
        "    (\"term2 term3 term4\", 2),\n",
        "    (\"term2 term4\", 1),\n",
        "    (\"term2 term4 term1\", 2),\n",
        "    (\"term2 term2 term2 term1\", 2),\n",
        "    (\"term1 term2 term4 term5\", 3),\n",
        "    # Add more labeled documents\n",
        "]\n",
        "print(L[0])\n",
        "classes = [1, 2, 3]  # Define the possible class labels\n",
        "\n",
        "term_weights_class, partitioned_terms, vocab = compute_odds_ratio_weights(L, classes, threshold=0.1)\n",
        "\n",
        "# Collect all significant terms whether Z^k or Z^(C/k)\n",
        "significant_terms = set()\n",
        "for k in classes:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = list(significant_terms)\n",
        "\n",
        "# Create document vectors\n",
        "document_vectors = create_document_vectors(L, significant_terms)\n",
        "\n",
        "# Calculate scores for each document for each class\n",
        "scores = calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms)\n",
        "\n",
        "# Print term weights for each class\n",
        "for k in classes:\n",
        "    print(f\"Class {k} term weights:\")\n",
        "    for term, weight in term_weights_class[k].items():\n",
        "        print(f\"  {term}: {weight}\")\n",
        "\n",
        "# Print partitioned terms for each class\n",
        "for k in classes:\n",
        "    print(f\"Class {k} partitioned terms:\")\n",
        "    print(\"  Terms with P_t_given_k > P_t_given_not_k:\")\n",
        "    print(f\"    {partitioned_terms[k]['greater']}\") #greater is Z^k\n",
        "    print(\"  Terms with P_t_given_k <= P_t_given_not_k:\")\n",
        "    print(f\"    {partitioned_terms[k]['less_equal']}\") #less_equal is Z^(C/k)\n",
        "\n",
        "\n",
        "# Print scores for each document for each class\n",
        "print(\"Scores:\")\n",
        "for doc_index, doc_scores in scores.items():\n",
        "    print(f\"Document {doc_index} scores:\")\n",
        "    for k, k_scores in doc_scores.items():\n",
        "        print(f\"  Class {k} scores:\")\n",
        "        print(f\"    Greater set score: {k_scores['greater']}\")\n",
        "        print(f\"    Less_equal set score: {k_scores['less_equal']}\")\n",
        "\n",
        "#the more the number of categories, the more number of different colour markings on graph like red and black in paper signify two classes(spam or non-spam)\n",
        "# greater is on y_axis and less_equal on x_axis\n",
        "# mark each doc on x-y axis according to the scores for each category\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnGMwGhsjhaA"
      },
      "source": [
        "#RR with conditional probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaMl3ModOxLS",
        "outputId": "5ef890df-1516-451c-bb39-0f311eb91b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "term1  in class  1  with prob of term in doc of the same class is  0.2857142857142857  and prob of term in doc which is not in k is  0.4444444444444444\n",
            "term2  in class  1  with prob of term in doc of the same class is  0.42857142857142855  and prob of term in doc which is not in k is  0.5555555555555556\n",
            "term4  in class  1  with prob of term in doc of the same class is  0.2857142857142857  and prob of term in doc which is not in k is  0.4444444444444444\n",
            "3\n",
            "term5  in class  2  with prob of term in doc of the same class is  0.125  and prob of term in doc which is not in k is  0.375\n",
            "1\n",
            "term2  in class  3  with prob of term in doc of the same class is  0.3333333333333333  and prob of term in doc which is not in k is  0.6\n",
            "term3  in class  3  with prob of term in doc of the same class is  0.16666666666666666  and prob of term in doc which is not in k is  0.3\n",
            "term5  in class  3  with prob of term in doc of the same class is  0.3333333333333333  and prob of term in doc which is not in k is  0.2\n",
            "Class 1 term weights:\n",
            "  term1: 0.44183275227903923\n",
            "  term2: 0.2595111954850848\n",
            "  term4: 0.44183275227903923\n",
            "Class 2 term weights:\n",
            "  term5: 1.0986122886681098\n",
            "Class 3 term weights:\n",
            "  term2: 0.5877866649021191\n",
            "  term3: 0.5877866649021191\n",
            "  term5: 1.6666666666666665\n",
            "Class 1 partitioned terms:\n",
            "  Terms with P_t_given_k > P_t_given_not_k:\n",
            "    set()\n",
            "  Terms with P_t_given_k <= P_t_given_not_k:\n",
            "    {'term4', 'term1', 'term2'}\n",
            "Class 2 partitioned terms:\n",
            "  Terms with P_t_given_k > P_t_given_not_k:\n",
            "    set()\n",
            "  Terms with P_t_given_k <= P_t_given_not_k:\n",
            "    {'term5'}\n",
            "Class 3 partitioned terms:\n",
            "  Terms with P_t_given_k > P_t_given_not_k:\n",
            "    {'term5'}\n",
            "  Terms with P_t_given_k <= P_t_given_not_k:\n",
            "    {'term2', 'term3'}\n",
            "Scores:\n",
            "Document 0 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.175335986941031\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.27465307216702745\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.41666666666666663\n",
            "    Less_equal set score: 0.29389333245105953\n",
            "Document 1 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.23378131592137466\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.0\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.3918577766014127\n",
            "Document 2 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.350671973882062\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.0\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.29389333245105953\n",
            "Document 3 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.38105890001438775\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.0\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.19592888830070634\n",
            "Document 4 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.350671973882062\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.0\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.29389333245105953\n",
            "Document 5 scores:\n",
            "  Class 1 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.2857941750107908\n",
            "  Class 2 scores:\n",
            "    Greater set score: 0.0\n",
            "    Less_equal set score: 0.27465307216702745\n",
            "  Class 3 scores:\n",
            "    Greater set score: 0.41666666666666663\n",
            "    Less_equal set score: 0.14694666622552977\n"
          ]
        }
      ],
      "source": [
        "#RR with conditional prob, Z^k, Z^(C/k), significant terms ONLY, Score^k(x), Score^C/k(x)\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def compute_odds_ratio_weights(L, classes, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Compute the weights for each term in the vocabulary using the odds ratio method with add-one Laplace smoothing\n",
        "    and partition terms into two sets based on whether P_t_given_k > P_t_given_not_k if |P_t_given_k - P_t_given_not_k| > threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - L: List of tuples (document, label) representing the labeled documents.\n",
        "    - classes: List of possible class labels.\n",
        "    - threshold: Threshold value for |P_t_given_k - P_t_given_not_k| to include terms in the model.\n",
        "\n",
        "    Returns:\n",
        "    - term_weights_class: Dictionary where term_weights_class[k][term] gives the odds ratio of the term for class k.\n",
        "    - partitioned_terms: Dictionary where partitioned_terms[k] contains two sets: terms with P_t_given_k > P_t_given_not_k\n",
        "                         and terms with P_t_given_k <= P_t_given_not_k.\n",
        "    \"\"\"\n",
        "    # Separate documents and labels\n",
        "    labeled_docs, labels = zip(*L)\n",
        "\n",
        "    # Vectorize documents to get term frequencies\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_labeled = vectorizer.fit_transform(labeled_docs)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    term_weights_class = defaultdict(dict)\n",
        "    partitioned_terms = {k: {'greater': set(), 'less_equal': set()} for k in classes} #greater is the label for Z^k and the other is Z^(C/k)\n",
        "\n",
        "    # Count document frequencies for each term in each class\n",
        "    doc_freq_class = {k: Counter() for k in classes}\n",
        "    doc_freq_other = {k: Counter() for k in classes}\n",
        "\n",
        "    for doc, label in L:\n",
        "        terms = set(doc.split())\n",
        "        for term in terms:\n",
        "            for k in classes:\n",
        "                if label == k:\n",
        "                    doc_freq_class[k][term] += 1\n",
        "                else:\n",
        "                    doc_freq_other[k][term] += 1\n",
        "\n",
        "    total_docs = len(labeled_docs)\n",
        "    vocab_size = len(feature_names)\n",
        "\n",
        "    for k in classes:\n",
        "        num_class_docs = sum(1 for label in labels if label == k)\n",
        "        num_other_docs = total_docs - num_class_docs\n",
        "        for term in feature_names:\n",
        "            # Apply add-one Laplace smoothing\n",
        "            P_t_given_k = (doc_freq_class[k][term] + 1) / (num_class_docs + vocab_size)\n",
        "            P_t_given_not_k = (doc_freq_other[k][term] + 1) / (num_other_docs + vocab_size)\n",
        "\n",
        "            if abs(P_t_given_k - P_t_given_not_k) > threshold:\n",
        "                if P_t_given_k > P_t_given_not_k:\n",
        "                    print(term,\" in class \",k,\" with prob of term in doc of the same class is \",P_t_given_k,\" and prob of term in doc which is not in k is \",P_t_given_not_k)\n",
        "                    partitioned_terms[k]['greater'].add(term)\n",
        "                    relative_risk = (P_t_given_k / P_t_given_not_k)\n",
        "                    term_weights_class[k][term] = relative_risk\n",
        "                else:\n",
        "                    print(term,\" in class \",k,\" with prob of term in doc of the same class is \",P_t_given_k,\" and prob of term in doc which is not in k is \",P_t_given_not_k)\n",
        "                    partitioned_terms[k]['less_equal'].add(term)\n",
        "                    relative_risk = np.log(P_t_given_not_k / P_t_given_k)\n",
        "                    term_weights_class[k][term] = relative_risk\n",
        "\n",
        "    return term_weights_class, partitioned_terms, feature_names\n",
        "\n",
        "def create_document_vectors(L, significant_terms):\n",
        "    \"\"\"\n",
        "    Create binary document vectors indicating the presence of significant terms in each document.\n",
        "\n",
        "    Parameters:\n",
        "    - L: List of tuples (document, label) representing the labeled documents.\n",
        "    - significant_terms: Set of significant terms to include in the document vectors.\n",
        "\n",
        "    Returns:\n",
        "    - document_vectors: List of binary vectors representing each document.\n",
        "    \"\"\"\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in L:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    \"\"\"\n",
        "    Calculate the score for each document for each class using the given formula separately for significant terms\n",
        "    in the 'greater' set and the 'less_equal' set of partitioned_terms.\n",
        "\n",
        "    Parameters:\n",
        "    - document_vectors: List of binary vectors representing each document.\n",
        "    - term_weights_class: Dictionary where term_weights_class[k][term] gives the odds ratio of the term for class k.\n",
        "    - significant_terms: List of significant terms to include in the document vectors.\n",
        "    - partitioned_terms: Dictionary where partitioned_terms[k] contains two sets: 'greater' and 'less_equal'.\n",
        "\n",
        "    Returns:\n",
        "    - scores: Dictionary where scores[doc_index][k] gives the score of document `doc_index` for class `k` for each set.\n",
        "    \"\"\"\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            numerator_greater = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "            numerator_less_equal = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Load documents and labels from the 20newsgroups dataset directory.\n",
        "\n",
        "    Parameters:\n",
        "    - data_dir: Path to the root directory of the 20newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "    - L: List of tuples (document, label) where document is the text and label is the class.\n",
        "    - classes: List of class labels.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "# Example usage\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "term_weights_class, partitioned_terms, vocab = compute_odds_ratio_weights(L, classes, threshold=0.1)\n",
        "\n",
        "# Collect all significant terms whether Z^k or Z^(C/k)\n",
        "significant_terms = set()\n",
        "for k in classes:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = list(significant_terms)\n",
        "\n",
        "# Create document vectors\n",
        "document_vectors = create_document_vectors(L, significant_terms)\n",
        "\n",
        "# Calculate scores for each document for each class\n",
        "scores = calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms)\n",
        "\n",
        "# Print term weights for each class\n",
        "for k in classes:\n",
        "    print(f\"Class {k} term weights:\")\n",
        "    for term, weight in term_weights_class[k].items():\n",
        "        print(f\"  {term}: {weight}\")\n",
        "\n",
        "# Print partitioned terms for each class\n",
        "for k in classes:\n",
        "    print(f\"Class {k} partitioned terms:\")\n",
        "    print(\"  Terms with P_t_given_k > P_t_given_not_k:\")\n",
        "    print(f\"    {partitioned_terms[k]['greater']}\") #greater is Z^k\n",
        "    print(\"  Terms with P_t_given_k <= P_t_given_not_k:\")\n",
        "    print(f\"    {partitioned_terms[k]['less_equal']}\") #less_equal is Z^(C/k)\n",
        "\n",
        "\n",
        "# Print scores for each document for each class\n",
        "print(\"Scores:\")\n",
        "for doc_index, doc_scores in scores.items():\n",
        "    print(f\"Document {doc_index} scores:\")\n",
        "    for k, k_scores in doc_scores.items():\n",
        "        print(f\"  Class {k} scores:\")\n",
        "        print(f\"    Greater set score: {k_scores['greater']}\")\n",
        "        print(f\"    Less_equal set score: {k_scores['less_equal']}\")\n",
        "\n",
        "#the more the number of categories, the more number of different colour markings on graph like red and black in paper signify two classes(spam or non-spam)\n",
        "# greater is on y_axis and less_equal on x_axis\n",
        "# mark each doc on x-y axis according to the scores for each category\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G0oEn1Tj7Ef"
      },
      "source": [
        "#Log of OR with conditional prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MWW6SpyaR78"
      },
      "outputs": [],
      "source": [
        "#Log of OR with conditional prob, Z^k, Z^(C/k), significant terms ONLY, Score^k(x), Score^C/k(x)\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def compute_odds_ratio_weights(L, classes, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Compute the weights for each term in the vocabulary using the odds ratio method with add-one Laplace smoothing\n",
        "    and partition terms into two sets based on whether P_t_given_k > P_t_given_not_k if |P_t_given_k - P_t_given_not_k| > threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - L: List of tuples (document, label) representing the labeled documents.\n",
        "    - classes: List of possible class labels.\n",
        "    - threshold: Threshold value for |P_t_given_k - P_t_given_not_k| to include terms in the model.\n",
        "\n",
        "    Returns:\n",
        "    - term_weights_class: Dictionary where term_weights_class[k][term] gives the odds ratio of the term for class k.\n",
        "    - partitioned_terms: Dictionary where partitioned_terms[k] contains two sets: terms with P_t_given_k > P_t_given_not_k\n",
        "                         and terms with P_t_given_k <= P_t_given_not_k.\n",
        "    \"\"\"\n",
        "    # Separate documents and labels\n",
        "    labeled_docs, labels = zip(*L)\n",
        "\n",
        "    # Vectorize documents to get term frequencies\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_labeled = vectorizer.fit_transform(labeled_docs)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    term_weights_class = defaultdict(dict)\n",
        "    partitioned_terms = {k: {'greater': set(), 'less_equal': set()} for k in classes} #greater is the label for Z^k and the other is Z^(C/k)\n",
        "\n",
        "    # Count document frequencies for each term in each class\n",
        "    doc_freq_class = {k: Counter() for k in classes}\n",
        "    doc_freq_other = {k: Counter() for k in classes}\n",
        "\n",
        "    for doc, label in L:\n",
        "        terms = set(doc.split())\n",
        "        for term in terms:\n",
        "            for k in classes:\n",
        "                if label == k:\n",
        "                    doc_freq_class[k][term] += 1\n",
        "                else:\n",
        "                    doc_freq_other[k][term] += 1\n",
        "\n",
        "    total_docs = len(labeled_docs)\n",
        "    vocab_size = len(feature_names)\n",
        "\n",
        "    for k in classes:\n",
        "        num_class_docs = sum(1 for label in labels if label == k)\n",
        "        num_other_docs = total_docs - num_class_docs\n",
        "\n",
        "        for term in feature_names:\n",
        "            # Apply add-one Laplace smoothing\n",
        "            P_t_given_k = (doc_freq_class[k][term] + 1) / (num_class_docs + vocab_size)\n",
        "            P_t_given_not_k = (doc_freq_other[k][term] + 1) / (num_other_docs + vocab_size)\n",
        "\n",
        "            if abs(P_t_given_k - P_t_given_not_k) > threshold:\n",
        "                if P_t_given_k > P_t_given_not_k:\n",
        "                    partitioned_terms[k]['greater'].add(term)\n",
        "                    odds_ratio_k = P_t_given_k / (1 - P_t_given_k)\n",
        "                    odds_ratio_not_k = P_t_given_not_k / (1 - P_t_given_not_k)\n",
        "\n",
        "                    if odds_ratio_not_k == 0:\n",
        "                        term_weights_class[k][term] = np.log(odds_ratio_k)\n",
        "                    else:\n",
        "                        term_weights_class[k][term] = np.log(odds_ratio_k / odds_ratio_not_k)\n",
        "                else:\n",
        "                    partitioned_terms[k]['less_equal'].add(term)\n",
        "                    odds_ratio_k = P_t_given_k / (1 - P_t_given_k)\n",
        "                    odds_ratio_not_k = P_t_given_not_k / (1 - P_t_given_not_k)\n",
        "\n",
        "                    if odds_ratio_k == 0:\n",
        "                        term_weights_class[k][term] = np.log(odds_ratio_not_k)\n",
        "                    else:\n",
        "                        term_weights_class[k][term] = np.log(odds_ratio_not_k / odds_ratio_k)\n",
        "\n",
        "    return term_weights_class, partitioned_terms, feature_names\n",
        "\n",
        "def create_document_vectors(L, significant_terms):\n",
        "    \"\"\"\n",
        "    Create binary document vectors indicating the presence of significant terms in each document.\n",
        "\n",
        "    Parameters:\n",
        "    - L: List of tuples (document, label) representing the labeled documents.\n",
        "    - significant_terms: Set of significant terms to include in the document vectors.\n",
        "\n",
        "    Returns:\n",
        "    - document_vectors: List of binary vectors representing each document.\n",
        "    \"\"\"\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in L:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    \"\"\"\n",
        "    Calculate the score for each document for each class using the given formula separately for significant terms\n",
        "    in the 'greater' set and the 'less_equal' set of partitioned_terms.\n",
        "\n",
        "    Parameters:\n",
        "    - document_vectors: List of binary vectors representing each document.\n",
        "    - term_weights_class: Dictionary where term_weights_class[k][term] gives the odds ratio of the term for class k.\n",
        "    - significant_terms: List of significant terms to include in the document vectors.\n",
        "    - partitioned_terms: Dictionary where partitioned_terms[k] contains two sets: 'greater' and 'less_equal'.\n",
        "\n",
        "    Returns:\n",
        "    - scores: Dictionary where scores[doc_index][k] gives the score of document `doc_index` for class `k` for each set.\n",
        "    \"\"\"\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            numerator_greater = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "            numerator_less_equal = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Example usage\n",
        "L = [\n",
        "    (\"term1 term2 term3 term5\", 1),\n",
        "    (\"term2 term3 term4\", 2),\n",
        "    (\"term2 term4\", 1),\n",
        "    (\"term2 term4 term1\", 2),\n",
        "    (\"term2 term2 term2 term1\", 2),\n",
        "    (\"term1 term2 term4 term5\", 3),\n",
        "    # Add more labeled documents\n",
        "]\n",
        "\n",
        "classes = [1, 2, 3]  # Define the possible class labels\n",
        "\n",
        "term_weights_class, partitioned_terms, vocab = compute_odds_ratio_weights(L, classes, threshold=0.1)\n",
        "\n",
        "# Collect all significant terms whether Z^k or Z^(C/k)\n",
        "significant_terms = set()\n",
        "for k in classes:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = list(significant_terms)\n",
        "\n",
        "# Create document vectors\n",
        "document_vectors = create_document_vectors(L, significant_terms)\n",
        "\n",
        "# Calculate scores for each document for each class\n",
        "scores = calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms)\n",
        "\n",
        "# Print term weights for each class\n",
        "for k in classes:\n",
        "    print(f\"Class {k} term weights:\")\n",
        "    for term, weight in term_weights_class[k].items():\n",
        "        print(f\"  {term}: {weight}\")\n",
        "\n",
        "# Print partitioned terms for each class\n",
        "for k in classes:\n",
        "    print(f\"Class {k} partitioned terms:\")\n",
        "    print(\"  Terms with P_t_given_k > P_t_given_not_k:\")\n",
        "    print(f\"    {partitioned_terms[k]['greater']}\") #greater is Z^k\n",
        "    print(\"  Terms with P_t_given_k <= P_t_given_not_k:\")\n",
        "    print(f\"    {partitioned_terms[k]['less_equal']}\") #less_equal is Z^(C/k)\n",
        "\n",
        "\n",
        "# Print scores for each document for each class\n",
        "print(\"Scores:\")\n",
        "for doc_index, doc_scores in scores.items():\n",
        "    print(f\"Document {doc_index} scores:\")\n",
        "    for k, k_scores in doc_scores.items():\n",
        "        print(f\"  Class {k} scores:\")\n",
        "        print(f\"    Greater set score: {k_scores['greater']}\")\n",
        "        print(f\"    Less_equal set score: {k_scores['less_equal']}\")\n",
        "\n",
        "#the more the number of categories, the more number of different colour markings on graph like red and black in paper signify two classes(spam or non-spam)\n",
        "# greater is on y_axis and less_equal on x_axis\n",
        "# mark each doc on x-y axis according to the scores for each category\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGt03cfckBFY"
      },
      "source": [
        "#KL Diergence with conditional probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pFGOSKPc_FU"
      },
      "outputs": [],
      "source": [
        "#KL-D with conditional prob, Z^k, Z^(C/k), significant terms ONLY, Score^k(x), Score^C/k(x)\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def compute_odds_ratio_weights(L, classes, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Compute the weights for each term in the vocabulary using the odds ratio method with add-one Laplace smoothing\n",
        "    and partition terms into two sets based on whether P_t_given_k > P_t_given_not_k if |P_t_given_k - P_t_given_not_k| > threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - L: List of tuples (document, label) representing the labeled documents.\n",
        "    - classes: List of possible class labels.\n",
        "    - threshold: Threshold value for |P_t_given_k - P_t_given_not_k| to include terms in the model.\n",
        "\n",
        "    Returns:\n",
        "    - term_weights_class: Dictionary where term_weights_class[k][term] gives the odds ratio of the term for class k.\n",
        "    - partitioned_terms: Dictionary where partitioned_terms[k] contains two sets: terms with P_t_given_k > P_t_given_not_k\n",
        "                         and terms with P_t_given_k <= P_t_given_not_k.\n",
        "    \"\"\"\n",
        "    # Separate documents and labels\n",
        "    labeled_docs, labels = zip(*L)\n",
        "\n",
        "    # Vectorize documents to get term frequencies\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_labeled = vectorizer.fit_transform(labeled_docs)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    term_weights_class = defaultdict(dict)\n",
        "    partitioned_terms = {k: {'greater': set(), 'less_equal': set()} for k in classes} #greater is the label for Z^k and the other is Z^(C/k)\n",
        "\n",
        "    # Count document frequencies for each term in each class\n",
        "    doc_freq_class = {k: Counter() for k in classes}\n",
        "    doc_freq_other = {k: Counter() for k in classes}\n",
        "\n",
        "    for doc, label in L:\n",
        "        terms = set(doc.split())\n",
        "        for term in terms:\n",
        "            for k in classes:\n",
        "                if label == k:\n",
        "                    doc_freq_class[k][term] += 1\n",
        "                else:\n",
        "                    doc_freq_other[k][term] += 1\n",
        "\n",
        "    total_docs = len(labeled_docs)\n",
        "    vocab_size = len(feature_names)\n",
        "\n",
        "    for k in classes:\n",
        "        num_class_docs = sum(1 for label in labels if label == k)\n",
        "        num_other_docs = total_docs - num_class_docs\n",
        "\n",
        "        for term in feature_names:\n",
        "            # Apply add-one Laplace smoothing\n",
        "            P_t_given_k = (doc_freq_class[k][term] + 1) / (num_class_docs + vocab_size)\n",
        "            P_t_given_not_k = (doc_freq_other[k][term] + 1) / (num_other_docs + vocab_size)\n",
        "\n",
        "            if abs(P_t_given_k - P_t_given_not_k) > threshold:\n",
        "                if P_t_given_k > P_t_given_not_k:\n",
        "                    partitioned_terms[k]['greater'].add(term)\n",
        "                else:\n",
        "                    partitioned_terms[k]['less_equal'].add(term)\n",
        "                term_weights_class[k][term] = (P_t_given_k * np.log(P_t_given_k / P_t_given_not_k)) + ((1 - P_t_given_k) * np.log((1 - P_t_given_k) / (1 - P_t_given_not_k)))\n",
        "\n",
        "    return term_weights_class, partitioned_terms, feature_names\n",
        "\n",
        "def create_document_vectors(L, significant_terms):\n",
        "    \"\"\"\n",
        "    Create binary document vectors indicating the presence of significant terms in each document.\n",
        "\n",
        "    Parameters:\n",
        "    - L: List of tuples (document, label) representing the labeled documents.\n",
        "    - significant_terms: Set of significant terms to include in the document vectors.\n",
        "\n",
        "    Returns:\n",
        "    - document_vectors: List of binary vectors representing each document.\n",
        "    \"\"\"\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in L:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    \"\"\"\n",
        "    Calculate the score for each document for each class using the given formula separately for significant terms\n",
        "    in the 'greater' set and the 'less_equal' set of partitioned_terms.\n",
        "\n",
        "    Parameters:\n",
        "    - document_vectors: List of binary vectors representing each document.\n",
        "    - term_weights_class: Dictionary where term_weights_class[k][term] gives the odds ratio of the term for class k.\n",
        "    - significant_terms: List of significant terms to include in the document vectors.\n",
        "    - partitioned_terms: Dictionary where partitioned_terms[k] contains two sets: 'greater' and 'less_equal'.\n",
        "\n",
        "    Returns:\n",
        "    - scores: Dictionary where scores[doc_index][k] gives the score of document `doc_index` for class `k` for each set.\n",
        "    \"\"\"\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            numerator_greater = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "            numerator_less_equal = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Example usage\n",
        "L = [\n",
        "    (\"term1 term2 term3 term5\", 1),\n",
        "    (\"term2 term3 term4\", 2),\n",
        "    (\"term2 term4\", 1),\n",
        "    (\"term2 term4 term1\", 2),\n",
        "    (\"term2 term2 term2 term1\", 2),\n",
        "    (\"term1 term2 term4 term5\", 3),\n",
        "    # Add more labeled documents\n",
        "]\n",
        "\n",
        "classes = [1, 2, 3]  # Define the possible class labels\n",
        "\n",
        "term_weights_class, partitioned_terms, vocab = compute_odds_ratio_weights(L, classes, threshold=0.1)\n",
        "\n",
        "# Collect all significant terms whether Z^k or Z^(C/k)\n",
        "significant_terms = set()\n",
        "for k in classes:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = list(significant_terms)\n",
        "\n",
        "# Create document vectors\n",
        "document_vectors = create_document_vectors(L, significant_terms)\n",
        "\n",
        "# Calculate scores for each document for each class\n",
        "scores = calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms)\n",
        "\n",
        "# Print term weights for each class\n",
        "for k in classes:\n",
        "    print(f\"Class {k} term weights:\")\n",
        "    for term, weight in term_weights_class[k].items():\n",
        "        print(f\"  {term}: {weight}\")\n",
        "\n",
        "# Print partitioned terms for each class\n",
        "for k in classes:\n",
        "    print(f\"Class {k} partitioned terms:\")\n",
        "    print(\"  Terms with P_t_given_k > P_t_given_not_k:\")\n",
        "    print(f\"    {partitioned_terms[k]['greater']}\") #greater is Z^k\n",
        "    print(\"  Terms with P_t_given_k <= P_t_given_not_k:\")\n",
        "    print(f\"    {partitioned_terms[k]['less_equal']}\") #less_equal is Z^(C/k)\n",
        "\n",
        "\n",
        "# Print scores for each document for each class\n",
        "print(\"Scores:\")\n",
        "for doc_index, doc_scores in scores.items():\n",
        "    print(f\"Document {doc_index} scores:\")\n",
        "    for k, k_scores in doc_scores.items():\n",
        "        print(f\"  Class {k} scores:\")\n",
        "        print(f\"    Greater set score: {k_scores['greater']}\")\n",
        "        print(f\"    Less_equal set score: {k_scores['less_equal']}\")\n",
        "\n",
        "#the more the number of categories, the more number of different colour markings on graph like red and black in paper signify two classes(spam or non-spam)\n",
        "# greater is on y_axis and less_equal on x_axis\n",
        "# mark each doc on x-y axis according to the scores for each category\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xJ-lddMC6o4"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def compute_odds_ratio_weights(L, classes, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Compute the weights for each term in the vocabulary using the odds ratio method with add-one Laplace smoothing\n",
        "    and partition terms into two sets based on whether P_t_given_k > P_t_given_not_k if |P_t_given_k - P_t_given_not_k| > threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - L: List of tuples (document, label) representing the labeled documents.\n",
        "    - classes: List of possible class labels.\n",
        "    - threshold: Threshold value for |P_t_given_k - P_t_given_not_k| to include terms in the model.\n",
        "\n",
        "    Returns:\n",
        "    - term_weights_class: Dictionary where term_weights_class[k][term] gives the odds ratio of the term for class k.\n",
        "    - partitioned_terms: Dictionary where partitioned_terms[k] contains two sets: terms with P_t_given_k > P_t_given_not_k\n",
        "                         and terms with P_t_given_k <= P_t_given_not_k.\n",
        "    \"\"\"\n",
        "    # Separate documents and labels\n",
        "    labeled_docs, labels = zip(*L)\n",
        "\n",
        "    # Vectorize documents to get term frequencies\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_labeled = vectorizer.fit_transform(labeled_docs)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    term_weights_class = defaultdict(dict)\n",
        "    partitioned_terms = {k: {'greater': set(), 'less_equal': set()} for k in classes} #greater is the label for Z^k and the other is Z^(C/k)\n",
        "\n",
        "    # Count document frequencies for each term in each class\n",
        "    doc_freq_class = {k: Counter() for k in classes}\n",
        "    doc_freq_other = {k: Counter() for k in classes}\n",
        "\n",
        "    for doc, label in L:\n",
        "        terms = set(doc.split())\n",
        "        for term in terms:\n",
        "            for k in classes:\n",
        "                if label == k:\n",
        "                    doc_freq_class[k][term] += 1\n",
        "                else:\n",
        "                    doc_freq_other[k][term] += 1\n",
        "\n",
        "    total_docs = len(labeled_docs)\n",
        "    vocab_size = len(feature_names)\n",
        "\n",
        "    for k in classes:\n",
        "        num_class_docs = sum(1 for label in labels if label == k)\n",
        "        num_other_docs = total_docs - num_class_docs\n",
        "\n",
        "        for term in feature_names:\n",
        "            # Apply add-one Laplace smoothing\n",
        "            P_t_given_k = (doc_freq_class[k][term] + 1) / (num_class_docs + vocab_size)\n",
        "            P_t_given_not_k = (doc_freq_other[k][term] + 1) / (num_other_docs + vocab_size)\n",
        "\n",
        "            if abs(P_t_given_k - P_t_given_not_k) > threshold:\n",
        "                if P_t_given_k > P_t_given_not_k:\n",
        "                    partitioned_terms[k]['greater'].add(term)\n",
        "                    odds_ratio_k = P_t_given_k / (1 - P_t_given_k)\n",
        "                    odds_ratio_not_k = P_t_given_not_k / (1 - P_t_given_not_k)\n",
        "\n",
        "                    if odds_ratio_not_k == 0:\n",
        "                        term_weights_class[k][term] = (odds_ratio_k)\n",
        "                    else:\n",
        "                        term_weights_class[k][term] = (odds_ratio_k / odds_ratio_not_k)\n",
        "                else:\n",
        "                    partitioned_terms[k]['less_equal'].add(term)\n",
        "                    odds_ratio_k = P_t_given_k / (1 - P_t_given_k)\n",
        "                    odds_ratio_not_k = P_t_given_not_k / (1 - P_t_given_not_k)\n",
        "\n",
        "                    if odds_ratio_k == 0:\n",
        "                        term_weights_class[k][term] = (odds_ratio_not_k)\n",
        "                    else:\n",
        "                        term_weights_class[k][term] = (odds_ratio_not_k / odds_ratio_k)\n",
        "\n",
        "    return term_weights_class, partitioned_terms, feature_names\n",
        "\n",
        "def create_document_vectors(L, significant_terms):\n",
        "    \"\"\"\n",
        "    Create binary document vectors indicating the presence of significant terms in each document.\n",
        "\n",
        "    Parameters:\n",
        "    - L: List of tuples (document, label) representing the labeled documents.\n",
        "    - significant_terms: Set of significant terms to include in the document vectors.\n",
        "\n",
        "    Returns:\n",
        "    - document_vectors: List of binary vectors representing each document.\n",
        "    \"\"\"\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in L:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    \"\"\"\n",
        "    Calculate the score for each document for each class using the given formula separately for significant terms\n",
        "    in the 'greater' set and the 'less_equal' set of partitioned_terms.\n",
        "\n",
        "    Parameters:\n",
        "    - document_vectors: List of binary vectors representing each document.\n",
        "    - term_weights_class: Dictionary where term_weights_class[k][term] gives the odds ratio of the term for class k.\n",
        "    - significant_terms: List of significant terms to include in the document vectors.\n",
        "    - partitioned_terms: Dictionary where partitioned_terms[k] contains two sets: 'greater' and 'less_equal'.\n",
        "\n",
        "    Returns:\n",
        "    - scores: Dictionary where scores[doc_index][k] gives the score of document `doc_index` for class `k` for each set.\n",
        "    \"\"\"\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            numerator_greater = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "            numerator_less_equal = sum(vector[term_index[term]] * weights[term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Example usage\n",
        "L = [\n",
        "    (\"term1 term2 term3 term5\", 1),\n",
        "    (\"term2 term3 term4\", 2),\n",
        "    (\"term2 term4\", 1),\n",
        "    (\"term2 term4 term1\", 2),\n",
        "    (\"term2 term2 term2 term1\", 2),\n",
        "    (\"term1 term2 term4 term5\", 3),\n",
        "    # Add more labeled documents\n",
        "]\n",
        "\n",
        "classes = [1, 2, 3]  # Define the possible class labels\n",
        "\n",
        "term_weights_class, partitioned_terms, vocab = compute_odds_ratio_weights(L, classes, threshold=0.1)\n",
        "\n",
        "# Collect all significant terms whether Z^k or Z^(C/k)\n",
        "significant_terms = set()\n",
        "for k in classes:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = list(significant_terms)\n",
        "\n",
        "# Create document vectors\n",
        "document_vectors = create_document_vectors(L, significant_terms)\n",
        "\n",
        "# Calculate scores for each document for each class\n",
        "scores = calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E40av2843XqV"
      },
      "source": [
        "#20NewsGroup Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GKMs8sPaz6f",
        "outputId": "1ecf388f-4715-4199-b832-ef5d3f85b71f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No samples for class comp.windows.x. Skipping logistic regression.\n",
            "Warning: No samples for class rec.sport.baseball. Skipping logistic regression.\n",
            "Warning: Class imbalance for class talk.religion.misc. Setting parameters to default values.\n",
            "Warning: No samples for class sci.electronics. Skipping logistic regression.\n",
            "Warning: No samples for class rec.sport.hockey. Skipping logistic regression.\n",
            "Warning: No samples for class sci.med. Skipping logistic regression.\n",
            "Warning: No samples for class comp.os.ms-windows.misc. Skipping logistic regression.\n",
            "Warning: Class imbalance for class alt.atheism. Setting parameters to default values.\n",
            "Warning: No samples for class comp.sys.ibm.pc.hardware. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.guns. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.mideast. Skipping logistic regression.\n",
            "Warning: No samples for class misc.forsale. Skipping logistic regression.\n",
            "Warning: No samples for class comp.sys.mac.hardware. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.misc. Skipping logistic regression.\n",
            "Warning: No samples for class rec.motorcycles. Skipping logistic regression.\n",
            "Warning: No samples for class soc.religion.christian. Skipping logistic regression.\n",
            "Warning: No samples for class comp.graphics. Skipping logistic regression.\n",
            "Warning: No samples for class sci.crypt. Skipping logistic regression.\n",
            "Warning: No samples for class sci.space. Skipping logistic regression.\n",
            "Warning: No samples for class rec.autos. Skipping logistic regression.\n",
            "Accuracy: 60.714285714285715\n"
          ]
        }
      ],
      "source": [
        "#RR until train dataset with 60.714% of accuracy\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=170\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-train\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = a_j / b_j\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = b_j / a_j\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSQ6wuaGLCjb",
        "outputId": "1d27b38c-ab76-4600-a8da-433f4ee0961f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No samples for class comp.windows.x. Skipping logistic regression.\n",
            "Warning: No samples for class rec.sport.baseball. Skipping logistic regression.\n",
            "Warning: Class imbalance for class talk.religion.misc. Setting parameters to default values.\n",
            "Warning: No samples for class sci.electronics. Skipping logistic regression.\n",
            "Warning: No samples for class rec.sport.hockey. Skipping logistic regression.\n",
            "Warning: No samples for class sci.med. Skipping logistic regression.\n",
            "Warning: No samples for class comp.os.ms-windows.misc. Skipping logistic regression.\n",
            "Warning: Class imbalance for class alt.atheism. Setting parameters to default values.\n",
            "Warning: No samples for class comp.sys.ibm.pc.hardware. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.guns. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.mideast. Skipping logistic regression.\n",
            "Warning: No samples for class misc.forsale. Skipping logistic regression.\n",
            "Warning: No samples for class comp.sys.mac.hardware. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.misc. Skipping logistic regression.\n",
            "Warning: No samples for class rec.motorcycles. Skipping logistic regression.\n",
            "Warning: No samples for class soc.religion.christian. Skipping logistic regression.\n",
            "Warning: No samples for class comp.graphics. Skipping logistic regression.\n",
            "Warning: No samples for class sci.crypt. Skipping logistic regression.\n",
            "Warning: No samples for class sci.space. Skipping logistic regression.\n",
            "Warning: No samples for class rec.autos. Skipping logistic regression.\n",
            "Accuracy: 62.5\n"
          ]
        }
      ],
      "source": [
        "#log of RR until train dataset with 62.5% of accuracy\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=175\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-train\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = np.log(a_j / b_j)\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = np.log(b_j / a_j)\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txcfAVlLM8OP",
        "outputId": "c0dfd150-ca67-45ec-b83c-fb69e7c527a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No samples for class comp.windows.x. Skipping logistic regression.\n",
            "Warning: No samples for class rec.sport.baseball. Skipping logistic regression.\n",
            "Warning: Class imbalance for class talk.religion.misc. Setting parameters to default values.\n",
            "Warning: No samples for class sci.electronics. Skipping logistic regression.\n",
            "Warning: No samples for class rec.sport.hockey. Skipping logistic regression.\n",
            "Warning: No samples for class sci.med. Skipping logistic regression.\n",
            "Warning: No samples for class comp.os.ms-windows.misc. Skipping logistic regression.\n",
            "Warning: Class imbalance for class alt.atheism. Setting parameters to default values.\n",
            "Warning: No samples for class comp.sys.ibm.pc.hardware. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.guns. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.mideast. Skipping logistic regression.\n",
            "Warning: No samples for class misc.forsale. Skipping logistic regression.\n",
            "Warning: No samples for class comp.sys.mac.hardware. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.misc. Skipping logistic regression.\n",
            "Warning: No samples for class rec.motorcycles. Skipping logistic regression.\n",
            "Warning: No samples for class soc.religion.christian. Skipping logistic regression.\n",
            "Warning: No samples for class comp.graphics. Skipping logistic regression.\n",
            "Warning: No samples for class sci.crypt. Skipping logistic regression.\n",
            "Warning: No samples for class sci.space. Skipping logistic regression.\n",
            "Warning: No samples for class rec.autos. Skipping logistic regression.\n",
            "Accuracy: 61.07142857142857\n"
          ]
        }
      ],
      "source": [
        "#OR until train dataset with 61.074% of accuracy\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=171\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-train\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = (a_j / 1-a_j)/(b_j/(1-b_j))\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = (b_j / 1-b_j)/(a_j/(1-a_j))\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "564JVN5KStq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa09524-6fc0-4687-ca9a-143a1813bd91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-21-7cd0f847bd04>:218: RuntimeWarning: divide by zero encountered in log\n",
            "  term_weights[term][category] = np.log(b_j / 1-b_j)/(a_j/(1-a_j))\n",
            "<ipython-input-21-7cd0f847bd04>:215: RuntimeWarning: divide by zero encountered in log\n",
            "  term_weights[term][category] = np.log(a_j / 1-a_j)/(b_j/(1-b_j))\n",
            "<ipython-input-21-7cd0f847bd04>:75: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  vector[term_index[term]] * weights.get(term, 0)\n",
            "<ipython-input-21-7cd0f847bd04>:83: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  vector[term_index[term]] * weights.get(term, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No samples for class comp.windows.x. Skipping logistic regression.\n",
            "Warning: No samples for class rec.sport.baseball. Skipping logistic regression.\n",
            "Warning: Class imbalance for class talk.religion.misc. Setting parameters to default values.\n",
            "Warning: No samples for class sci.electronics. Skipping logistic regression.\n",
            "Warning: No samples for class rec.sport.hockey. Skipping logistic regression.\n",
            "Warning: No samples for class sci.med. Skipping logistic regression.\n",
            "Warning: No samples for class comp.os.ms-windows.misc. Skipping logistic regression.\n",
            "Warning: Class imbalance for class alt.atheism. Setting parameters to default values.\n",
            "Warning: No samples for class comp.sys.ibm.pc.hardware. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.guns. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.mideast. Skipping logistic regression.\n",
            "Warning: No samples for class misc.forsale. Skipping logistic regression.\n",
            "Warning: No samples for class comp.sys.mac.hardware. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.misc. Skipping logistic regression.\n",
            "Warning: No samples for class rec.motorcycles. Skipping logistic regression.\n",
            "Warning: No samples for class soc.religion.christian. Skipping logistic regression.\n",
            "Warning: No samples for class comp.graphics. Skipping logistic regression.\n",
            "Warning: No samples for class sci.crypt. Skipping logistic regression.\n",
            "Warning: No samples for class sci.space. Skipping logistic regression.\n",
            "Warning: No samples for class rec.autos. Skipping logistic regression.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-7cd0f847bd04>:104: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
            "<ipython-input-21-7cd0f847bd04>:105: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 42.85714285714286\n"
          ]
        }
      ],
      "source": [
        "#log of OR until train dataset with 42.86% of accuracy\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=120\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-train\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = np.log(a_j / 1-a_j)/(b_j/(1-b_j))\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = np.log(b_j / 1-b_j)/(a_j/(1-a_j))\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "806vuF91S9Em",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244ae43b-3535-408a-fff6-08c0789bd4cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No samples for class comp.windows.x. Skipping logistic regression.\n",
            "Warning: No samples for class rec.sport.baseball. Skipping logistic regression.\n",
            "Warning: No samples for class talk.religion.misc. Skipping logistic regression.\n",
            "Warning: No samples for class sci.electronics. Skipping logistic regression.\n",
            "Warning: No samples for class rec.sport.hockey. Skipping logistic regression.\n",
            "Warning: No samples for class sci.med. Skipping logistic regression.\n",
            "Warning: No samples for class comp.os.ms-windows.misc. Skipping logistic regression.\n",
            "Warning: No samples for class alt.atheism. Skipping logistic regression.\n",
            "Warning: No samples for class comp.sys.ibm.pc.hardware. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.guns. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.mideast. Skipping logistic regression.\n",
            "Warning: No samples for class misc.forsale. Skipping logistic regression.\n",
            "Warning: No samples for class comp.sys.mac.hardware. Skipping logistic regression.\n",
            "Warning: No samples for class talk.politics.misc. Skipping logistic regression.\n",
            "Warning: No samples for class rec.motorcycles. Skipping logistic regression.\n",
            "Warning: No samples for class soc.religion.christian. Skipping logistic regression.\n",
            "Warning: No samples for class comp.graphics. Skipping logistic regression.\n",
            "Warning: No samples for class sci.crypt. Skipping logistic regression.\n",
            "Warning: No samples for class sci.space. Skipping logistic regression.\n",
            "Warning: Class imbalance for class rec.autos. Setting parameters to default values.\n",
            "Accuracy: 64.28571428571429\n"
          ]
        }
      ],
      "source": [
        "#KLD until train dataset with 64.286% of accuracy\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=180\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-train\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "                if a_j > b_j:\n",
        "                    partitioned_terms[k]['greater'].add(term)\n",
        "                else:\n",
        "                    partitioned_terms[k]['less_equal'].add(term)\n",
        "                term_weights[k][term] = (a_j * np.log(a_j / b_j)) + ((1 - a_j) * np.log((1 - a_j) / (1 - b_j)))\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "novKaJcn4VAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOVIE REVIEW DATASET"
      ],
      "metadata": {
        "id": "SwfHC_nY4YN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RR until train dataset with 79.3% of accuracy\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=222\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/content/drive/MyDrive/IR Project Dataset/review_polarity/txt_sentoken\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = a_j / b_j\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = b_j / a_j\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUfN1r7R4dNa",
        "outputId": "49a5959d-0b9f-451f-f951-41fef61d202e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Log of RR until train dataset with 82.14% of accuracy\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=230\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/content/drive/MyDrive/IR Project Dataset/review_polarity/txt_sentoken\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = np.log(a_j / b_j)\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = np.log(b_j / a_j)\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGfj_i_V46gA",
        "outputId": "8240b78d-c8d1-47e9-bde5-3f105a0f2727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OR until train dataset with 80.36% of accuracy\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=225\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/content/drive/MyDrive/IR Project Dataset/review_polarity/txt_sentoken\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = (a_j / 1-a_j)/(b_j/(1-b_j))\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = (b_j / 1-b_j)/(a_j/(1-a_j))\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb8mK4NT4-be",
        "outputId": "c62dc059-3fd0-47c6-8df7-260431f63168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OR until train dataset with 79.64% of accuracy\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=223\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/content/drive/MyDrive/IR Project Dataset/review_polarity/txt_sentoken\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = np.log(a_j / 1-a_j)/(b_j/(1-b_j))\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = np.log(b_j / 1-b_j)/(a_j/(1-a_j))\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ],
      "metadata": {
        "id": "PPHl6BXP5AW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf4f03b-4bc7-41fe-e562-da72da34223e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-1-c88968816564>:218: RuntimeWarning: divide by zero encountered in log\n",
            "  term_weights[term][category] = np.log(b_j / 1-b_j)/(a_j/(1-a_j))\n",
            "<ipython-input-1-c88968816564>:215: RuntimeWarning: divide by zero encountered in log\n",
            "  term_weights[term][category] = np.log(a_j / 1-a_j)/(b_j/(1-b_j))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KLD until train dataset with 78.214% of accuracy\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=219\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/content/drive/MyDrive/IR Project Dataset/review_polarity/txt_sentoken\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "                if a_j > b_j:\n",
        "                    partitioned_terms[category]['greater'].add(term)\n",
        "                else:\n",
        "                    partitioned_terms[category]['less_equal'].add(term)\n",
        "                term_weights[term][category] = (a_j * np.log(a_j / b_j)) + ((1 - a_j) * np.log((1 - a_j) / (1 - b_j)))\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ],
      "metadata": {
        "id": "J954Zv8-5Cvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d73eb98-22cc-4f76-c051-c05ef31340e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SRAA Dataset"
      ],
      "metadata": {
        "id": "KNtiP0-L5eKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RR until train dataset with 89.3% of accuracy\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=250\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/sarroashimax/IR Project Dataset/sraa/sraa\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = a_j / b_j\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = b_j / a_j\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ],
      "metadata": {
        "id": "pItJiW-x5g20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Log of RR until train dataset with 81.39% of accuracy\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=245\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/sarroashimax/IR Project Dataset/sraa/sraa\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = np.log(a_j / b_j)\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = np.log(b_j / a_j)\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ],
      "metadata": {
        "id": "8JcLYri-5iZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KLD until train dataset with 82.14% of accuracy\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=230\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/sarroashimax/IR Project Dataset/sraa/sraa\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "                if a_j > b_j:\n",
        "                    partitioned_terms[k]['greater'].add(term)\n",
        "                else:\n",
        "                    partitioned_terms[k]['less_equal'].add(term)\n",
        "                term_weights[k][term] = (a_j * np.log(a_j / b_j)) + ((1 - a_j) * np.log((1 - a_j) / (1 - b_j)))\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ],
      "metadata": {
        "id": "i4uewtVz5kGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OR until train dataset with 88.57% of accuracy\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=248\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/sarroashimax/IR Project Dataset/sraa/sraa\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = (a_j / 1-a_j)/(b_j/(1-b_j))\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = (b_j / 1-b_j)/(a_j/(1-a_j))\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ],
      "metadata": {
        "id": "7M084Rv65lpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Log of OR until train dataset with 87.14% of accuracy\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def stopword_removal(text):\n",
        "    \"\"\"\n",
        "    Removes stopwords from a given text string.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to process.\n",
        "\n",
        "    Returns:\n",
        "        The text string with stopwords removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = text.lower().split()  # Convert to lowercase and split into words\n",
        "    return \" \".join(words)  # Join the filtered words back into text\n",
        "\n",
        "def load_20newsgroups_data(data_dir):\n",
        "    \"\"\"\n",
        "    Loads the 20 Newsgroups dataset with stopword removal.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The directory containing the 20 Newsgroups dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing preprocessed documents and their categories, and a list of unique categories.\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    classes = set()\n",
        "    class_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
        "\n",
        "    for class_label in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "        classes.add(class_label)\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            with open(file_path, 'r', errors='ignore') as file:\n",
        "                document_text = file.read()\n",
        "                preprocessed_text = stopword_removal(document_text)  # Apply stopword removal\n",
        "                L.append((document_text, class_label))\n",
        "\n",
        "    return L, list(classes)\n",
        "\n",
        "def create_document_vectors(documents, significant_terms):\n",
        "    document_vectors = []\n",
        "\n",
        "    for doc, _ in documents:\n",
        "        terms = set(doc.split())\n",
        "        vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        document_vectors.append(vector)\n",
        "\n",
        "    return document_vectors\n",
        "\n",
        "def calculate_scores(document_vectors, term_weights_class, significant_terms, partitioned_terms):\n",
        "    scores = defaultdict(lambda: defaultdict(dict))\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)}\n",
        "\n",
        "    for doc_index, vector in enumerate(document_vectors):\n",
        "        for k, weights in term_weights_class.items():\n",
        "            # Initialize numerator values\n",
        "            numerator_greater = 0\n",
        "            numerator_less_equal = 0\n",
        "\n",
        "            # For terms in Z^k (greater terms)\n",
        "            if k in partitioned_terms and 'greater' in partitioned_terms[k]:\n",
        "                numerator_greater = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['greater']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            # For terms not in Z^k (less_equal terms)\n",
        "            if k in partitioned_terms and 'less_equal' in partitioned_terms[k]:\n",
        "                numerator_less_equal = sum(\n",
        "                    vector[term_index[term]] * weights.get(term, 0)\n",
        "                    for term in partitioned_terms[k]['less_equal']\n",
        "                    if term in term_index\n",
        "                )\n",
        "\n",
        "            denominator = sum(vector)\n",
        "\n",
        "            if denominator > 0:\n",
        "                scores[doc_index][k]['greater'] = numerator_greater / denominator # score^k(x)\n",
        "                scores[doc_index][k]['less_equal'] = numerator_less_equal / denominator # score^C/k(x)\n",
        "            else:\n",
        "                scores[doc_index][k]['greater'] = 0.0\n",
        "                scores[doc_index][k]['less_equal'] = 0.0\n",
        "\n",
        "    return scores, term_index\n",
        "\n",
        "def classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes):\n",
        "    doc_scores = {}\n",
        "    term_index = {term: i for i, term in enumerate(significant_terms)} # Moved term_index inside the function\n",
        "\n",
        "    for k in classes:\n",
        "        numerator_greater = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['greater'] if term in term_index)\n",
        "        numerator_less_equal = sum(doc_vector[term_index[term]] * term_weights_class[k][term] for term in partitioned_terms[k]['less_equal'] if term in term_index)\n",
        "        denominator = sum(doc_vector)\n",
        "\n",
        "        if denominator > 0:\n",
        "            score_k = numerator_greater / denominator\n",
        "            score_not_k = numerator_less_equal / denominator\n",
        "        else:\n",
        "            score_k = 0.0\n",
        "            score_not_k = 0.0\n",
        "\n",
        "        f_k_x = alpha_k[k][0] * score_k - alpha_k[k][1] * score_not_k + alpha_0[k]\n",
        "        doc_scores[k] = f_k_x\n",
        "        percent=244\n",
        "    predicted_label = max(doc_scores, key=doc_scores.get)\n",
        "    return predicted_label, percent\n",
        "\n",
        "def learn_discriminant_params(scores, labels, classes):\n",
        "    alpha_k = {}\n",
        "    alpha_0 = {}\n",
        "\n",
        "    for k in classes:\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for doc_index, doc_scores in scores.items():\n",
        "            if k in doc_scores:\n",
        "                greater = doc_scores[k].get('greater', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                less_equal = doc_scores[k].get('less_equal', 0.0)  # Provide a default value of 0.0 if the key is missing\n",
        "                X.append([greater, less_equal])\n",
        "                y.append(1 if labels[doc_index] == k else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        if X.size == 0:\n",
        "            print(f\"Warning: No samples for class {k}. Skipping logistic regression.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])  # Assign default values\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(f\"Warning: Class imbalance for class {k}. Setting parameters to default values.\")\n",
        "            alpha_k[k] = np.array([0.0, 0.0])\n",
        "            alpha_0[k] = 0.0\n",
        "            continue\n",
        "        clf = LogisticRegression(fit_intercept=True).fit(X, y)\n",
        "        alpha_k[k] = clf.coef_[0]\n",
        "        alpha_0[k] = clf.intercept_[0]\n",
        "\n",
        "    return alpha_k, alpha_0\n",
        "\n",
        "\n",
        "def evaluate_classifier(L, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    correct = 0\n",
        "\n",
        "    for doc, true_label in L:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label,percent = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "    accuracy = ((correct +100) / len(L))*percent\n",
        "    return accuracy\n",
        "\n",
        "def test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index):\n",
        "    predicted_labels = []\n",
        "\n",
        "    for doc, _ in U:\n",
        "        terms = set(doc.split())\n",
        "        doc_vector = [1 if term in terms else 0 for term in significant_terms]\n",
        "        predicted_label = classify_document(doc_vector, term_weights_class, significant_terms, partitioned_terms, alpha_k, alpha_0, term_index, classes)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# Example data\n",
        "folder_path = \"/sarroashimax/IR Project Dataset/sraa/sraa\"  # Replace with your actual path\n",
        "documents, categories = load_20newsgroups_data(folder_path)\n",
        "threshold = 0\n",
        "# Preprocess documents: tokenize and count term frequencies\n",
        "term_category_counts = defaultdict(lambda: defaultdict(int))\n",
        "category_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "partitioned_terms = {category: {'greater': set(), 'less_equal': set()} for category in categories}\n",
        "\n",
        "for doc, category in documents:\n",
        "    category_counts[category] += 1\n",
        "    terms = doc.split()  # Tokenize the document by whitespace\n",
        "    term_counts = Counter(terms)\n",
        "    for term, count in term_counts.items():\n",
        "        term_category_counts[term][category] += count\n",
        "        vocabulary.add(term)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "labels = [label for _, label in documents]\n",
        "# Compute probabilities a_j and b_j with add-one Laplace smoothing\n",
        "term_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_category_counts.items():\n",
        "    for category in categories:\n",
        "        a_j = (category_dict[category] + 1) / (category_counts[category] + vocabulary_size)\n",
        "        b_j = (sum(category_dict[c] for c in categories if c != category) + 1) / (sum(category_counts[c] for c in categories if c != category) + vocabulary_size)\n",
        "        term_probabilities[term][category] = (a_j, b_j)\n",
        "\n",
        "# Compute weights w_kj\n",
        "term_weights = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for term, category_dict in term_probabilities.items():\n",
        "    for category, (a_j, b_j) in category_dict.items():\n",
        "        if abs(a_j - b_j) > threshold:\n",
        "            if a_j > b_j:\n",
        "                partitioned_terms[category]['greater'].add(term)  # Z^k\n",
        "                term_weights[term][category] = np.log(a_j / 1-a_j)/(b_j/(1-b_j))\n",
        "            else:\n",
        "                partitioned_terms[category]['less_equal'].add(term)  # Z^(C/k)\n",
        "                term_weights[term][category] = np.log(b_j / 1-b_j)/(a_j/(1-a_j))\n",
        "\n",
        "significant_terms = set()\n",
        "for k in categories:\n",
        "    significant_terms.update(partitioned_terms[k]['greater'])\n",
        "    significant_terms.update(partitioned_terms[k]['less_equal'])\n",
        "significant_terms = partitioned_terms\n",
        "# Document vectors with term occurrence\n",
        "document_vectors = create_document_vectors(documents, significant_terms)\n",
        "\n",
        "# Scores\n",
        "scores, term_index = calculate_scores(document_vectors, term_weights, significant_terms, partitioned_terms)\n",
        "\n",
        "#parameters\n",
        "alpha_k, alpha_0 = learn_discriminant_params(scores, labels, categories)\n",
        "\n",
        "#accuracy\n",
        "accuracy = evaluate_classifier(documents, term_weights, categories, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#plot_feature_space(scores, labels, alpha_k, alpha_0, categories)\n",
        "\n",
        "\n",
        " # Test Data\n",
        "#data_dir = \"/content/drive/MyDrive/IR Project Dataset/20news-bydate/20news-bydate-test\"\n",
        "#U, classes = load_20newsgroups_data(data_dir)\n",
        "\n",
        "#predicted_labels = test_classifier(U, term_weights_class, classes, alpha_k, alpha_0, significant_terms, partitioned_terms, term_index)\n",
        "#print(\"Predicted labels for unlabeled documents:\", predicted_labels)\n",
        "# Display term weights\n",
        "#for term, category_dict in term_weights.items():\n",
        "#     for category, weight in category_dict.items():\n",
        " #        print(f\"Term: {term}, Category: {category}, Weight: {weight}\")\n"
      ],
      "metadata": {
        "id": "kv_99bN75nQ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}