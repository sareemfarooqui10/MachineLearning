# -*- coding: utf-8 -*-
"""Tensorflow_introduction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r6nrJhX83hq2MWl3SAoDfCFgeuIBASIq
"""

import h5py
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.python.framework.ops import EagerTensor
from tensorflow.python.ops.resource_variable_ops import ResourceVariable
import time

"""<a name='2'></a>
## 2 - Basic Optimization with GradientTape


"""

train_dataset = h5py.File('/content/train.hdf5', "r")
test_dataset = h5py.File('/content/test.hdf5', "r")
type(train_dataset)
print("Count of images in train dataset: ",train_dataset["image"].shape[0])
print("Count of images in test dataset: ",test_dataset["image"].shape[0])
print("Size of each image: ",train_dataset["image"].shape[1],"x",train_dataset["image"].shape[2],"x 3")
print("Train y size is ",train_dataset["label"].shape)

x_train = tf.data.Dataset.from_tensor_slices(train_dataset['image'])
y_train = tf.data.Dataset.from_tensor_slices(train_dataset['label'])

x_test = tf.data.Dataset.from_tensor_slices(test_dataset['image'])
y_test = tf.data.Dataset.from_tensor_slices(test_dataset['label'])

print(x_train.element_spec)
print("y train shape: ",tf.shape(list(y_train.as_numpy_iterator()))) # Convert the dataset to a list of NumPy arrays and then get the shape using tf.shape.
count=0
for i in (y_test):
  count+=1
print("Count of test images: ",count)

print(x_train.element_spec)

print(next(iter(x_train)))

"""The dataset that is used is a subset of the sign language digits. It contains ten different classes representing the digits from 0 to 9."""

unique_labels = set()
for element in y_train:
    unique_labels.add(element.numpy())
print(unique_labels)

"""Lets see some of the images in the dataset."""

images_iter = iter(x_train)
labels_iter = iter(y_train)
plt.figure(figsize=(10, 10))
for i in range(25):
    ax = plt.subplot(5, 5, i + 1)
    plt.imshow(next(images_iter).numpy().astype("uint8"))
    plt.title(next(labels_iter).numpy().astype("uint8"))
    plt.axis("off")

"""Transform TensorFlow dataset by invokig the `map` method to apply the function passed as an argument to each of the elements."""

import tensorflow as tf

def normalize(image):
    """
    Transform a grayscale image into a tensor of shape (28 * 28 * 3,)
    and normalize its components.

    Arguments:
    image - Tensor of shape (28, 28).

    Returns:
    result -- Transformed tensor of shape (28 * 28 * 3,)
    """
    print("Grayscale image shape: ",image.shape)

    # Reshape the image to (28, 28, 1) to add the channel dimension if it doesn't already have it
    if len(image.shape) == 2:
        image = tf.reshape(image, [image.shape[0],image.shape[1], 1])

    # Duplicate the single channel to create an RGB image
    image = tf.tile(image, [1, 1, 3])

    # Flatten the image tensor to shape (28 * 28 * 3,)
    image = tf.reshape(image, [-1])

    # Normalize the image
    image = tf.cast(image, tf.float32) / 255.0

    return image

new_train = x_train.map(normalize)
new_test = x_test.map(normalize)

print("size of each train image is: ",new_train.element_spec)
count=0
for i in new_train:
  count+=1
print("Count of train images: ",count)

print(new_train)

"""<a name='2-1'></a>
### 2.1 - Linear Function

"""

# GRADED FUNCTION: linear_function

def linear_function():
    """
    Implements a linear function:
            Initializes X to be a random tensor of shape (3,1)
            Initializes W to be a random tensor of shape (4,3)
            Initializes b to be a random tensor of shape (4,1)
    Returns:
    result -- Y = WX + b
    """

    np.random.seed(1)

    """
    Note, to ensure that the "random" numbers generated match the expected results,
    please create the variables in the order given in the starting code below.
    (Do not re-arrange the order).
    """

    X=tf.constant(np.random.randn(3,1))
    W=tf.constant(np.random.randn(4,3))
    b=tf.constant(np.random.randn(4,1))
    parameters={
        "X":X,
        "W":W,
        "b":b
    }
    Y=tf.add((tf.matmul(W,X)),b)

    return Y, parameters

result, parameters = linear_function()
print(parameters,"these were the X, W and b. Following is value Z",result)
print("Type of Z: ",(result.dtype))

print("\033[92mAll test passed")

"""**Expected Output**:

```
result =
[[-2.15657382]
 [ 2.95891446]
 [-1.08926781]
 [-0.84538042]]
```

<a name='2-2'></a>
### 2.2 - Computing the Sigmoid of z
Cast your tensor to type `float32` using `tf.cast`, then compute the sigmoid using `tf.keras.activations.sigmoid`.

<a name='ex-2'></a>
### Exercise 2 - sigmoid

Implement the sigmoid function below by using the following:

- `tf.cast("...", tf.float32)`
- `tf.keras.activations.sigmoid("...")`
"""

# GRADED FUNCTION: sigmoid

def sigmoid(z):

    """
    Computes the sigmoid of z

    Arguments:
    z -- input value, scalar or vector

    Returns:
    a -- (tf.float32) the sigmoid of z
    """
    # tf.keras.activations.sigmoid requires float16, float32, float64, complex64, or complex128.


    z=tf.cast(z,tf.float32)
    a=tf.keras.activations.sigmoid(z)

    return a

result = sigmoid(-1)
print ("type: " + str(type(result)))
print ("dtype: " + str(result.dtype))
print ("sigmoid(-1) = " + str(result))
print ("sigmoid(0) = " + str(sigmoid(0.0)))
print ("sigmoid(12) = " + str(sigmoid(12)))

print("\033[92mAll test passed")

"""**Expected Output**:
<table>
<tr>
<td>
type
</td>
<td>
class 'tensorflow.python.framework.ops.EagerTensor'
</td>
</tr><tr>
<td>
dtype
</td>
<td>
"dtype: 'float32'
</td>
</tr>
<tr>
<td>
Sigmoid(-1)
</td>
<td>
0.2689414
</td>
</tr>
<tr>
<td>
Sigmoid(0)
</td>
<td>
0.5
</td>
</tr>
<tr>
<td>
Sigmoid(12)
</td>
<td>
0.999994
</td>
</tr>

</table>

<a name='2-3'></a>
### 2.3 - Using One Hot Encodings


<img src="images/onehot.png" style="width:600px;height:150px;">

This is called "one hot" encoding, because in the converted representation, exactly one element of each column is "hot" (meaning set to 1).

- [tf.one_hot(labels, depth, axis=0)](https://www.tensorflow.org/api_docs/python/tf/one_hot)

`axis=0` indicates the new axis is created at dimension 0

<a name='ex-3'></a>
### Exercise 3 - one_hot_matrix

Implement the function below to take one label and the total number of classes $C$, and return the one hot encoding in a one-dimensional tensor (array). Use `tf.one_hot()` to do this, and `tf.reshape()` to reshape one hot tensor!

- `tf.reshape(tensor, shape)`
"""

# GRADED FUNCTION: one_hot_matrix
def one_hot_matrix(label, C=10):
    """
    Computes the one hot encoding for a single label

    Arguments:
        label --  (int) Categorical labels
        C --  (int) Number of different classes that label can take

    Returns:
         one_hot -- tf.Tensor A one-dimensional tensor (array) with the one hot encoding.
    """

    new_tf=tf.one_hot(label,C,axis=0)
    print("Before reshaping, tensor",new_tf," is of size: ",tf.shape(new_tf))
    one_hot=tf.reshape(new_tf,shape=[C])
    print("After reshaping, tensor",one_hot," is of size: ",tf.shape(one_hot))

    return one_hot

def one_hot_matrix_test(target):
    label = tf.constant(1)
    C = 4
    result = target(label, C)
    print("Test 1:",result)
    label_2 = [2]
    C = 5
    result = target(label_2, C)
    print("Test 2:", result))

    print("\033[92mAll test passed")

"""**Expected output**
```
Test 1: tf.Tensor([0. 1. 0. 0.], shape=(4,), dtype=float32)
Test 2: tf.Tensor([0. 0. 1. 0. 0.], shape=(5,), dtype=float32)
```
"""

for i in y_test:
  print(i) #this would print first hand sign value from test dataset
  break
new_y_test = y_test.map(one_hot_matrix)
new_y_train = y_train.map(one_hot_matrix)
print((new_y_train.element_spec))

print(next(iter(new_y_test))) #this prints one_hot encoding of the previous result i.e. 7

"""<a name='2-4'></a>
### 2.4 - Initialize the Parameters

The function we'll be calling is `tf.keras.initializers.GlorotNormal`, which draws samples from a truncated normal distribution centered on 0, with `stddev = sqrt(2 / (fan_in + fan_out))`, where `fan_in` is the number of input units and `fan_out` is the number of output units, both in the weight tensor.

<a name='ex-4'></a>
### Exercise 4 - initialize_parameters

Implement the function below to take in a shape and to return an array of numbers using the GlorotNormal initializer.

 - `tf.keras.initializers.GlorotNormal(seed=1)`
 - `tf.Variable(initializer(shape=())`
"""

# GRADED FUNCTION: initialize_parameters

def initialize_parameters():
    """
    Initializes parameters to build a neural network with TensorFlow. The shapes are:
                        W1 : [25, 2352]
                        b1 : [25, 1]
                        W2 : [12, 25]
                        b2 : [12, 1]
                        W3 : [6, 12]
                        b3 : [6, 1]

    Returns:
    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3
    """

    initializer = tf.keras.initializers.GlorotNormal(seed=1)
    W1=tf.Variable(initializer(shape=(25,2352)))
    b1=tf.Variable(initializer(shape=(25,1)))
    W2=tf.Variable(initializer(shape=(12,25)))
    b2=tf.Variable(initializer(shape=(12,1)))
    W3=tf.Variable(initializer(shape=(6,12)))
    b3=tf.Variable(initializer(shape=(6,1)))

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2,
                  "W3": W3,
                  "b3": b3}

    return parameters

def initialize_parameters_test(target):
    parameters = target()

    values = {"W1": (25, 2352), #28*28*3=2352
              "b1": (25, 1),
              "W2": (12, 25),
              "b2": (12, 1),
              "W3": (6, 12),
              "b3": (6, 1)}

    for key in parameters:
        print(f"{key} shape: {tuple(parameters[key].shape)}")

    print("\033[92mAll test passed")

initialize_parameters_test(initialize_parameters)

parameters = initialize_parameters()

"""<a name='3'></a>
## 3 - Building Your First Neural Network in TensorFlow

<a name='3-1'></a>
### 3.1 - Implement Forward Propagation


<a name='ex-5'></a>
### Exercise 5 - forward_propagation

Implement the `forward_propagation` function.

**Note** Use only the TF API.

- tf.math.add
- tf.linalg.matmul
- tf.keras.activations.relu
"""

# GRADED FUNCTION: forward_propagation

def forward_propagation(X, parameters):
    """
    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR

    Arguments:
    X -- input dataset placeholder, of shape (input size, number of examples)
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3"
                  the shapes are given in initialize_parameters

    Returns:
    Z3 -- the output of the last LINEAR unit
    """

    # Retrieve the parameters from the dictionary "parameters"
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    W3 = parameters['W3']
    b3 = parameters['b3']
    print("Transposed minibatch size: ",X.shape)
    Z1=tf.add(tf.matmul(W1,X),b1)
    A1=tf.keras.activations.relu(Z1)
    Z2=tf.add(tf.matmul(W2,A1),b2)
    A2=tf.keras.activations.relu(Z2)
    Z3=tf.add(tf.matmul(W3,A2),b3)

    return Z3

def forward_propagation_test(target, examples):
    minibatches = examples.batch(2) #takes only 2 samples for minibatch so minibatch_size=2
    print("minibatch coming from new_train size:",next(iter(minibatches))) #this must get transposed before forward propagation
    parametersk = initialize_parameters()
    W1 = parametersk['W1']
    b1 = parametersk['b1']
    W2 = parametersk['W2']
    b2 = parametersk['b2']
    W3 = parametersk['W3']
    b3 = parametersk['b3']
    index = 0
    minibatch = list(minibatches)[0]
    with tf.GradientTape() as tape:
        forward_pass = target(tf.transpose(minibatch), parametersk)
        print(forward_pass)
        fake_cost = tf.reduce_mean(forward_pass - np.ones((6,2)))
    index = index + 1
    trainable_variables = [W1, b1, W2, b2, W3, b3]
    grads = tape.gradient(fake_cost, trainable_variables)
    print("\033[92mAll test passed")
forward_propagation_test(forward_propagation, new_train)

"""**Expected output**
```
tf.Tensor(
[[-0.13430887  0.14086473]
 [ 0.21588647 -0.02582335]
 [ 0.7059658   0.6484556 ]
 [-1.1260961  -0.9329492 ]
 [-0.20181894 -0.3382722 ]
 [ 0.9558965   0.94167566]], shape=(6, 2), dtype=float32)
```

<a name='3-2'></a>
### 3.2 Compute the Total Loss

Define the loss function that we're going to use. For this case, since we have a classification problem with 6 labels, a categorical cross entropy will work!

<a name='ex-6'></a>
### Exercise 6 -  compute_total_loss

Implement the total loss function below. You will use it to compute the total loss of a batch of samples. With this convenient function, you can sum the losses across many batches, and divide the sum by the total number of samples to get the cost value.
"""

# GRADED FUNCTION: compute_total_loss

def compute_total_loss(logits, labels):
    """
    Computes the total loss

    Arguments:
    logits -- output of forward propagation (output of the last LINEAR unit), of shape (10, num_examples)
    labels -- "true" labels vector, same shape as Z3

    Returns:
    total_loss - Tensor of the total loss value
    """

    total_loss=tf.reduce_sum(tf.keras.losses.categorical_crossentropy(tf.transpose(labels),tf.transpose(logits),from_logits=True)) #NEED TENSORS IN TRANSPOSE
    return total_loss

def compute_total_loss_test(target, Y):
    pred = tf.constant([[ 2.4048107,   5.0334096 ],
             [-0.7921977,  -4.1523376 ],
             [ 0.9447198,  -0.46802214],
             [ 1.158121,    3.9810789 ],
             [ 4.768706,    2.3220146 ],
             [ 6.1481323,   3.909829  ],
             [ 6.1481323,   3.909829  ],[ 6.1481323,   3.909829  ],[ 6.1481323,   3.909829  ],[ 6.1481323,   3.909829  ]])
    minibatches = Y.batch(2)
    for minibatch in minibatches:
        result = target(pred, tf.transpose(minibatch))
        break

    print("Test 1: ", result)


    ### Test 2
    labels = tf.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])
    logits = tf.constant([[1., 0., 0.], [1., 0., 0.], [1., 0., 0.]])

    result = compute_total_loss(logits, labels)
    print("Test 2: ", result)

    print("\033[92mAll test passed")

compute_total_loss_test(compute_total_loss, new_y_train )

"""**Note:** When using sum of losses for gradient computation, it’s important to reduce the learning rate as the size of the mini-batch increases. This ensures that you don’t take large steps towards minimum.

<a name='3-3'></a>
### 3.3 - Train the Model

Specify the type of optimizer in one line, in this case `tf.keras.optimizers.Adam` (though we can use others such as SGD), and then call it within the training loop.

Notice the `tape.gradient` function: this allows us to retrieve the operations recorded for automatic differentiation inside the `GradientTape` block. Then, calling the optimizer method `apply_gradients`, will apply the optimizer's update rules to each trainable parameter. At the end of this assignment, we'll find some documentation that explains this more in detail, but for now, a simple explanation will do.
"""

def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,
          num_epochs = 1500, minibatch_size = 32, print_cost = True):
    """
    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.

    Arguments:
    X_train -- training set, of shape (input size = 2352, number of training examples = 6000)
    Y_train -- test set, of shape (output size = 10, number of training examples = 6000)
    X_test -- training set, of shape (input size = 2352, number of training examples = 1000)
    Y_test -- test set, of shape (output size = 10, number of test examples = 1000)
    learning_rate -- learning rate of the optimization
    num_epochs -- number of epochs of the optimization loop
    minibatch_size -- size of a minibatch
    print_cost -- True to print the cost every 10 epochs

    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """

    costs = []                                        # To keep track of the cost
    train_acc = []
    test_acc = []
    parameters = initialize_parameters()

    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    W3 = parameters['W3']
    b3 = parameters['b3']

    optimizer = tf.keras.optimizers.Adam(learning_rate)

    # The CategoricalAccuracy will track the accuracy for this multiclass problem
    test_accuracy = tf.keras.metrics.CategoricalAccuracy()
    train_accuracy = tf.keras.metrics.CategoricalAccuracy()

    dataset = tf.data.Dataset.zip((X_train, Y_train))
    test_dataset = tf.data.Dataset.zip((X_test, Y_test))

    # We can get the number of elements of a dataset using the cardinality method
    m = dataset.cardinality().numpy()

    minibatches = dataset.batch(minibatch_size).prefetch(8)
    test_minibatches = test_dataset.batch(minibatch_size).prefetch(8)
    X_train = X_train.batch(minibatch_size, drop_remainder=True).prefetch(8)# <<< extra step
    Y_train = Y_train.batch(minibatch_size, drop_remainder=True).prefetch(8) # loads memory faster

    # Do the training loop
    for epoch in range(num_epochs):

        epoch_total_loss = 0.

        #We need to reset object to start measuring from 0 the accuracy each epoch
        train_accuracy.reset_states()

        for (minibatch_X, minibatch_Y) in minibatches:
            with tf.GradientTape() as tape:
                # 1. predict
                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)

                # 2. loss
                minibatch_total_loss = compute_total_loss(Z3, tf.transpose(minibatch_Y)) #does total loss for an entire batch

            # We accumulate the accuracy of all the batches
            train_accuracy.update_state(minibatch_Y, tf.transpose(Z3))

            trainable_variables = [W1, b1, W2, b2, W3, b3]
            grads = tape.gradient(minibatch_total_loss, trainable_variables)
            optimizer.apply_gradients(zip(grads, trainable_variables)) #trainable variables updated
            epoch_total_loss += minibatch_total_loss

        # We divide the epoch total loss over the number of samples
        epoch_total_loss /= m

        # Print the cost every 10 epochs
        if print_cost == True and epoch % 10 == 0:
            print ("Cost after epoch %i: %f" % (epoch, epoch_total_loss))
            print("Train accuracy:", train_accuracy.result())

            # We evaluate the test set every 10 epochs to avoid computational overhead
            for (minibatch_X, minibatch_Y) in test_minibatches:
                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)
                test_accuracy.update_state(minibatch_Y, tf.transpose(Z3))
            print("Test_accuracy:", test_accuracy.result())

            costs.append(epoch_total_loss)
            train_acc.append(train_accuracy.result())
            test_acc.append(test_accuracy.result())
            test_accuracy.reset_states()


    return parameters, costs, train_acc, test_acc

parameters, costs, train_acc, test_acc = model(new_train, new_y_train, new_test, new_y_test, num_epochs=100)

"""**Expected output**

```
Cost after epoch 0: 1.830244
Train accuracy: tf.Tensor(0.17037037, shape=(), dtype=float32)
Test_accuracy: tf.Tensor(0.2, shape=(), dtype=float32)
Cost after epoch 10: 1.552390
Train accuracy: tf.Tensor(0.35925925, shape=(), dtype=float32)
Test_accuracy: tf.Tensor(0.30833334, shape=(), dtype=float32)
...
```
Make sure that the loss is going down and the accuracy going up!
"""

# Plot the cost
plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per fives)')
plt.title("Learning rate =" + str(0.0001))
plt.show()

# Plot the train accuracy
plt.plot(np.squeeze(train_acc))
plt.ylabel('Train Accuracy')
plt.xlabel('iterations (per fives)')
plt.title("Learning rate =" + str(0.0001))
# Plot the test accuracy
plt.plot(np.squeeze(test_acc))
plt.ylabel('Test Accuracy')
plt.xlabel('iterations (per fives)')
plt.title("Learning rate =" + str(0.0001))
plt.show()